version: '3.8'

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

x-default-healthcheck: &default-healthcheck
  test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s

x-default-deploy: &default-deploy
  replicas: 3
  update_config:
    parallelism: 1
    delay: 10s
    order: start-first
  restart_policy:
    condition: on-failure
    delay: 5s
    max_attempts: 3
    window: 120s
  resources:
    limits:
      cpus: '1'
      memory: 2G
    reservations:
      cpus: '0.5'
      memory: 1G

x-api-gateway-env: &api-gateway-env
  ENVIRONMENT: production
  DATABASE_URL: ${DATABASE_URL:-postgresql://user:password@postgres:5432/goat_prediction}
  REDIS_URL: ${REDIS_URL:-redis://:password@redis:6379/0}
  JWT_SECRET: ${JWT_SECRET}
  CORS_ORIGINS: ${CORS_ORIGINS:-https://app.goat-prediction.com,https://admin.goat-prediction.com}
  RATE_LIMIT_REQUESTS: ${RATE_LIMIT_REQUESTS:-100}
  RATE_LIMIT_PERIOD: ${RATE_LIMIT_PERIOD:-60}
  LOG_LEVEL: ${LOG_LEVEL:-INFO}
  SENTRY_DSN: ${SENTRY_DSN}
  NEW_RELIC_LICENSE_KEY: ${NEW_RELIC_LICENSE_KEY}
  NEW_RELIC_APP_NAME: ${NEW_RELIC_APP_NAME:-goat-prediction-api}
  PROMETHEUS_METRICS_PORT: ${PROMETHEUS_METRICS_PORT:-9100}

x-prediction-engine-env: &prediction-engine-env
  ENVIRONMENT: production
  POSTGRES_URL: ${POSTGRES_URL:-postgresql://user:password@postgres:5432/goat_prediction}
  TIMESCALE_URL: ${TIMESCALE_URL:-postgresql://timescale:password@timescaledb:5432/goat_prediction_timeseries}
  REDIS_URL: ${REDIS_URL:-redis://:password@redis:6379/0}
  KAFKA_BROKERS: ${KAFKA_BROKERS:-kafka:9092}
  ELASTICSEARCH_URL: ${ELASTICSEARCH_URL:-http://elasticsearch:9200}
  ML_MODELS_PATH: ${ML_MODELS_PATH:-/app/models}
  DATA_CACHE_PATH: ${DATA_CACHE_PATH:-/app/data/cache}
  LOG_LEVEL: ${LOG_LEVEL:-INFO}
  MODEL_REFRESH_INTERVAL: ${MODEL_REFRESH_INTERVAL:-3600}
  BATCH_PREDICTION_SIZE: ${BATCH_PREDICTION_SIZE:-100}
  PREDICTION_TIMEOUT: ${PREDICTION_TIMEOUT:-30}
  SENTRY_DSN: ${SENTRY_DSN}
  NEW_RELIC_LICENSE_KEY: ${NEW_RELIC_LICENSE_KEY}
  NEW_RELIC_APP_NAME: ${NEW_RELIC_APP_NAME:-goat-prediction-engine}
  PROMETHEUS_METRICS_PORT: ${PROMETHEUS_METRICS_PORT:-9101}

services:
  # Load Balancer / Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: goat-prediction-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./infrastructure/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./infrastructure/nginx/sites-available:/etc/nginx/sites-available
      - ./infrastructure/nginx/ssl:/etc/nginx/ssl
      - ./logs/nginx:/var/log/nginx
    depends_on:
      api-gateway:
        condition: service_healthy
      frontend:
        condition: service_healthy
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.2'
          memory: 256M
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging

  # API Gateway
  api-gateway:
    build:
      context: ./backend/api-gateway
      dockerfile: Dockerfile.prod
    container_name: goat-prediction-api-gateway
    expose:
      - "8000"
    environment: *api-gateway-env
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    deploy: *default-deploy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *default-logging
    volumes:
      - ./logs/api-gateway:/app/logs

  # Prediction Engine
  prediction-engine:
    build:
      context: ./backend/prediction-engine
      dockerfile: Dockerfile.prod
    container_name: goat-prediction-engine
    expose:
      - "8000"
    environment: *prediction-engine-env
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      timescaledb:
        condition: service_healthy
      kafka:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    deploy: *default-deploy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s
    logging: *default-logging
    volumes:
      - ./models/production:/app/models
      - ./data/cache:/app/data/cache
      - ./logs/prediction-engine:/app/logs
    command: >
      sh -c "
        python scripts/initialize_models.py &&
        gunicorn src.main:app \
          --workers 4 \
          --worker-class uvicorn.workers.UvicornWorker \
          --bind 0.0.0.0:8000 \
          --timeout 120 \
          --keep-alive 5 \
          --access-logfile - \
          --error-logfile - \
          --log-level info
      "

  # Frontend Web App
  frontend:
    build:
      context: ./frontend/web-app
      dockerfile: Dockerfile.prod
    container_name: goat-prediction-frontend
    expose:
      - "3000"
    environment:
      NODE_ENV: production
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-https://api.goat-prediction.com}
      NEXT_PUBLIC_WS_URL: ${NEXT_PUBLIC_WS_URL:-wss://ws.goat-prediction.com}
      NEXT_PUBLIC_SENTRY_DSN: ${NEXT_PUBLIC_SENTRY_DSN}
      NEXT_PUBLIC_GA_TRACKING_ID: ${NEXT_PUBLIC_GA_TRACKING_ID}
    deploy:
      replicas: 3
      update_config:
        parallelism: 2
        delay: 10s
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging

  # PostgreSQL Main Database
  postgres:
    image: postgres:15-alpine
    container_name: goat-prediction-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-goat_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-goat_prediction}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./database/supabase/migrations:/docker-entrypoint-initdb.d
      - ./database/supabase/backups:/backups
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-goat_user}"]
      interval: 30s
      timeout: 10s
      retries: 5
    logging: *default-logging
    command: >
      postgres
      -c max_connections=200
      -c shared_buffers=1GB
      -c effective_cache_size=3GB
      -c maintenance_work_mem=256MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100

  # TimescaleDB for Time-Series Data
  timescaledb:
    image: timescale/timescaledb:latest-pg15
    container_name: goat-prediction-timescaledb
    environment:
      POSTGRES_USER: ${TIMESCALE_USER:-timescale_user}
      POSTGRES_PASSWORD: ${TIMESCALE_PASSWORD}
      POSTGRES_DB: ${TIMESCALE_DB:-goat_prediction_timeseries}
    volumes:
      - timescaledb-data:/var/lib/postgresql/data
      - ./database/timescaledb:/docker-entrypoint-initdb.d
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${TIMESCALE_USER:-timescale_user}"]
      interval: 30s
      timeout: 10s
      retries: 5
    logging: *default-logging

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: goat-prediction-redis
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec
      --save 900 1
      --save 300 10
      --save 60 10000
    volumes:
      - redis-data:/data
      - ./database/redis/configurations/redis.conf:/usr/local/etc/redis/redis.conf
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 512M
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    logging: *default-logging

  # Kafka Cluster
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: goat-prediction-kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: ${KAFKA_BROKER_ID:-1}
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 10737418240
      KAFKA_MESSAGE_MAX_BYTES: 10485760
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760
      KAFKA_NUM_PARTITIONS: ${KAFKA_NUM_PARTITIONS:-10}
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 60s
      timeout: 20s
      retries: 3
    logging: *default-logging

  # Zookeeper for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: goat-prediction-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 512M
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging

  # Elasticsearch Cluster
  elasticsearch:
    image: elasticsearch:8.11.0
    container_name: goat-prediction-elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - cluster.routing.allocation.disk.threshold_enabled=true
      - cluster.routing.allocation.disk.watermark.low=85%
      - cluster.routing.allocation.disk.watermark.high=90%
      - cluster.routing.allocation.disk.watermark.flood_stage=95%
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 3
    logging: *default-logging

  # Monitoring Stack
  prometheus:
    image: prom/prometheus:latest
    container_name: goat-prediction-prometheus
    volumes:
      - ./infrastructure/monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
      - ./infrastructure/monitoring/prometheus/alerts:/etc/prometheus/alerts
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging

  grafana:
    image: grafana/grafana:latest
    container_name: goat-prediction-grafana
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_INSTALL_PLUGINS: grafana-piechart-panel,grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - grafana-data:/var/lib/grafana
      - ./infrastructure/monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./infrastructure/monitoring/dashboards:/var/lib/grafana/dashboards
    depends_on:
      - prometheus
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging

  alertmanager:
    image: prom/alertmanager:latest
    container_name: goat-prediction-alertmanager
    volumes:
      - ./infrastructure/monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.2'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging

  # MLflow for Model Management
  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    container_name: goat-prediction-mlflow
    environment:
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_ACCESS_KEY_ID: ${MINIO_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${MINIO_SECRET_KEY}
      MLFLOW_TRACKING_URI: postgresql://${POSTGRES_USER:-goat_user}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-goat_prediction}
    volumes:
      - mlflow-data:/mlflow
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging
    command: >
      mlflow server
      --backend-store-uri postgresql://${POSTGRES_USER:-goat_user}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-goat_prediction}
      --default-artifact-root s3://mlflow-artifacts/
      --host 0.0.0.0
      --port 5000

  # MinIO for Object Storage
  minio:
    image: minio/minio:latest
    container_name: goat-prediction-minio
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY}
      MINIO_BROWSER_REDIRECT_URL: ${MINIO_BROWSER_REDIRECT_URL}
    volumes:
      - minio-data:/data
    deploy:
      replicas: 4
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    logging: *default-logging
    command: server /data --console-address ":9001"

  # Certbot for SSL Certificates
  certbot:
    image: certbot/certbot:latest
    container_name: goat-prediction-certbot
    volumes:
      - ./infrastructure/nginx/ssl:/etc/letsencrypt
      - ./infrastructure/nginx/webroot:/var/www/certbot
    depends_on:
      - nginx
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;'"
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '0.2'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    logging: *default-logging

  # Backup Service
  backup:
    image: postgres:15-alpine
    container_name: goat-prediction-backup
    environment:
      POSTGRES_HOST: postgres
      POSTGRES_USER: ${POSTGRES_USER:-goat_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB:-goat_prediction}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      S3_BUCKET: ${BACKUP_S3_BUCKET}
      BACKUP_SCHEDULE: ${BACKUP_SCHEDULE:-0 2 * * *}
    volumes:
      - ./scripts/backup:/backup-scripts
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.2'
          memory: 256M
    command: >
      sh -c "
        echo 'Installing backup dependencies...' &&
        apk add --no-cache python3 py3-pip aws-cli &&
        pip3 install boto3 &&
        echo 'Starting backup service...' &&
        crond -f
      "
    logging: *default-logging

  # Log Shipper (Fluentd)
  fluentd:
    image: fluent/fluentd:v1.16-debian
    container_name: goat-prediction-fluentd
    volumes:
      - ./infrastructure/logging/fluentd.conf:/fluentd/etc/fluent.conf
      - ./logs:/var/log/goat-prediction
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.2'
          memory: 256M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:24220/api/plugins.json"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging: *default-logging

volumes:
  postgres-data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=nas.goat-prediction.com,rw
      device: ":/volumes/goat-prediction/postgres"
  
  timescaledb-data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=nas.goat-prediction.com,rw
      device: ":/volumes/goat-prediction/timescaledb"
  
  redis-data:
    driver: local
  
  elasticsearch-data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=nas.goat-prediction.com,rw
      device: ":/volumes/goat-prediction/elasticsearch"
  
  prometheus-data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=nas.goat-prediction.com,rw
      device: ":/volumes/goat-prediction/prometheus"
  
  grafana-data:
    driver: local
  
  alertmanager-data:
    driver: local
  
  mlflow-data:
    driver: local
  
  minio-data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=nas.goat-prediction.com,rw
      device: ":/volumes/goat-prediction/minio"

networks:
  default:
    name: goat-prediction-network
    driver: overlay
    attachable: true
    ipam:
      config:
        - subnet: 10.10.0.0/24
          gateway: 10.10.0.1
    driver_opts:
      encrypted: "true"
