version: '3.8'

services:
  # API Gateway - Main Service
  api-gateway:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: goat-api-gateway
    restart: unless-stopped
    ports:
      - "8000:8000"
      - "8001:8001"  # Metrics port
    environment:
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - REDIS_URL=redis://redis:6379/0
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - JWT_ALGORITHM=HS256
      - JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
      - JWT_REFRESH_TOKEN_EXPIRE_DAYS=7
      - CORS_ORIGINS=${CORS_ORIGINS}
      - RATE_LIMIT_PER_MINUTE=60
      - API_VERSION=v1
      - ENABLE_DOCS=true
      - ENABLE_METRICS=true
      - ENABLE_HEALTH_CHECKS=true
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - shared-tmp:/tmp
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      otel-collector:
        condition: service_started
    networks:
      - goat-network
      - monitoring-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "goat-api-gateway"

  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: goat-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init-scripts:/docker-entrypoint-initdb.d:ro
      - ./database/backups:/backups
    ports:
      - "5432:5432"
    networks:
      - goat-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: >
      postgres
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=4MB
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c max_worker_processes=4
      -c max_parallel_workers_per_gather=2
      -c max_parallel_workers=4
      -c max_parallel_maintenance_workers=2

  # Redis Cache & Pub/Sub
  redis:
    image: redis:7-alpine
    container_name: goat-redis
    restart: unless-stopped
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
      --appendonly yes
      --appendfsync everysec
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
    ports:
      - "6379:6379"
    networks:
      - goat-network
      - monitoring-network
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "3"

  # Redis Commander (Admin UI)
  redis-commander:
    image: rediscommander/redis-commander:latest
    container_name: goat-redis-commander
    restart: unless-stopped
    environment:
      - REDIS_HOSTS=local:redis:6379
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - HTTP_USER=${REDIS_COMMANDER_USER}
      - HTTP_PASSWORD=${REDIS_COMMANDER_PASSWORD}
    ports:
      - "8081:8081"
    depends_on:
      - redis
    networks:
      - goat-network
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  # PgAdmin (Admin UI for PostgreSQL)
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: goat-pgadmin
    restart: unless-stopped
    environment:
      - PGADMIN_DEFAULT_EMAIL=${PGADMIN_EMAIL}
      - PGADMIN_DEFAULT_PASSWORD=${PGADMIN_PASSWORD}
      - PGADMIN_CONFIG_SERVER_MODE=False
      - PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED=False
    volumes:
      - pgadmin_data:/var/lib/pgadmin
      - ./database/pgadmin-servers.json:/pgadmin4/servers.json:ro
    ports:
      - "8082:80"
    depends_on:
      - postgres
    networks:
      - goat-network
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: goat-otel-collector
    restart: unless-stopped
    command:
      - "--config=/etc/otel-collector-config.yaml"
    volumes:
      - ./monitoring/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
      - ./logs/otel:/var/log/otel
    ports:
      - "4317:4317"  # OTLP gRPC
      - "4318:4318"  # OTLP HTTP
      - "8888:8888"  # Prometheus metrics endpoint
      - "8889:8889"  # Prometheus exporter
      - "13133:13133" # Health check extension
      - "55679:55679" # zPages
    networks:
      - monitoring-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:13133"]
      interval: 30s
      timeout: 5s
      retries: 3

  # Prometheus for metrics
  prometheus:
    image: prom/prometheus:latest
    container_name: goat-prometheus
    restart: unless-stopped
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
      - ./monitoring/alerts:/etc/prometheus/alerts:ro
    ports:
      - "9090:9090"
    networks:
      - monitoring-network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Grafana for dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: goat-grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_DOMAIN=localhost
      - GF_SERVER_ROOT_URL=http://localhost:3000
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    networks:
      - monitoring-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: goat-jaeger
    restart: unless-stopped
    environment:
      - COLLECTOR_ZIPKIN_HTTP_PORT=9411
      - LOG_LEVEL=debug
    ports:
      - "16686:16686"  # UI
      - "14268:14268"  # Collector
      - "14250:14250"  # Collector gRPC
      - "9411:9411"    # Zipkin
    networks:
      - monitoring-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:16686"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Loki for logs
  loki:
    image: grafana/loki:latest
    container_name: goat-loki
    restart: unless-stopped
    ports:
      - "3100:3100"
    volumes:
      - loki_data:/loki
      - ./monitoring/loki-config.yaml:/etc/loki/local-config.yaml:ro
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - monitoring-network

  # Promtail for log collection
  promtail:
    image: grafana/promtail:latest
    container_name: goat-promtail
    restart: unless-stopped
    volumes:
      - ./logs:/var/log/goat
      - ./monitoring/promtail-config.yaml:/etc/promtail/config.yaml:ro
      - /var/log:/var/log/host:ro
    command: -config.file=/etc/promtail/config.yaml
    networks:
      - monitoring-network
    depends_on:
      - loki

  # Portainer for container management
  portainer:
    image: portainer/portainer-ce:latest
    container_name: goat-portainer
    restart: unless-stopped
    ports:
      - "9000:9000"
      - "9443:9443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - portainer_data:/data
    networks:
      - goat-network
    command: -H unix:///var/run/docker.sock
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "https://localhost:9443/api/status"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Traefik for reverse proxy (optional)
  traefik:
    image: traefik:v3.0
    container_name: goat-traefik
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"  # Dashboard
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./traefik/traefik.yml:/etc/traefik/traefik.yml:ro
      - ./traefik/config:/etc/traefik/config:ro
      - ./traefik/certs:/etc/traefik/certs:ro
    networks:
      - goat-network
      - monitoring-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.dashboard.rule=Host(`traefik.localhost`)"
      - "traefik.http.routers.dashboard.service=api@internal"
      - "traefik.http.routers.dashboard.middlewares=auth"
      - "traefik.http.middlewares.auth.basicauth.users=${TRAEFIK_AUTH_CREDENTIALS}"

  # Nginx for load balancing (alternative to Traefik)
  nginx:
    image: nginx:alpine
    container_name: goat-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/sites-available:/etc/nginx/sites-available:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./logs/nginx:/var/log/nginx
    depends_on:
      - api-gateway
    networks:
      - goat-network
      - monitoring-network
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MailHog for email testing
  mailhog:
    image: mailhog/mailhog:latest
    container_name: goat-mailhog
    restart: unless-stopped
    ports:
      - "1025:1025"  # SMTP
      - "8025:8025"  # Web UI
    networks:
      - goat-network

  # MinIO for object storage (S3 compatible)
  minio:
    image: minio/minio:latest
    container_name: goat-minio
    restart: unless-stopped
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_BROWSER=on
    ports:
      - "9001:9001"  # Console
      - "9000:9000"  # API
    volumes:
      - minio_data:/data
      - ./minio/config:/root/.minio:ro
    command: server /data --console-address ":9001"
    networks:
      - goat-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # Elasticsearch for advanced search
  elasticsearch:
    image: elasticsearch:8.11.0
    container_name: goat-elasticsearch
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - cluster.name=goat-prediction
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - goat-network
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s http://localhost:9200/_cluster/health | grep -q '\"status\":\"green\"'"
        ]
      interval: 30s
      timeout: 10s
      retries: 5

  # Kibana for Elasticsearch visualization
  kibana:
    image: kibana:8.11.0
    container_name: goat-kibana
    restart: unless-stopped
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    networks:
      - goat-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5601/api/status"]
      interval: 30s
      timeout: 10s
      retries: 3

# Named volumes for data persistence
volumes:
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/postgres
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/redis
  pgadmin_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/pgadmin
  grafana_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/grafana
  prometheus_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/prometheus
  loki_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/loki
  portainer_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/portainer
  minio_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/minio
  elasticsearch_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/elasticsearch
  shared-tmp:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./tmp

# Custom networks for isolation
networks:
  goat-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
    name: goat-prediction-network
  monitoring-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16
          gateway: 172.21.0.1
    name: goat-monitoring-network

# x-* extensions for reusable configurations
x-defaults: &defaults
  logging:
    driver: json-file
    options:
      max-size: "10m"
      max-file: "3"
      tag: "{{.Name}}"

x-healthcheck: &healthcheck-defaults
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 40s

x-environment: &environment-defaults
  environment:
    - NODE_ENV=production
    - TZ=UTC
    - LANG=C.UTF-8
    - LC_ALL=C.UTF-8

x-restart-policy: &restart-defaults
  restart: unless-stopped

x-volumes: &volumes-defaults
  volumes:
    - ./logs:/app/logs
    - ./config:/app/config:ro
    - shared-tmp:/tmp
