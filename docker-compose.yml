version: '3.8'

# ðŸš€ GOAT PREDICTION ULTIMATE - DOCKER COMPOSE CONFIGURATION
# Version: 1.0.0
# Author: GOAT Prediction Team
# Description: Production-ready multi-service orchestration for AI sports prediction platform

x-common-env: &common-env
  # Environment variables communes
  TZ: ${TZ:-UTC}
  NODE_ENV: ${NODE_ENV:-production}
  LOG_LEVEL: ${LOG_LEVEL:-info}
  # Configuration de sÃ©curitÃ©
  SECURITY_HEADERS: "enabled"
  RATE_LIMIT_ENABLED: "true"
  # Monitoring
  SENTRY_ENABLED: ${SENTRY_ENABLED:-true}
  SENTRY_DSN: ${SENTRY_DSN:-}
  # Database
  POSTGRES_HOST: postgres
  POSTGRES_PORT: 5432
  POSTGRES_USER: ${POSTGRES_USER:-goat_user}
  POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-goat_password}
  POSTGRES_DB: ${POSTGRES_DB:-goat_prediction}
  # Redis
  REDIS_HOST: redis
  REDIS_PORT: 6379
  REDIS_PASSWORD: ${REDIS_PASSWORD:-goat_redis_pass}
  # Supabase (alternative)
  SUPABASE_URL: ${SUPABASE_URL:-}
  SUPABASE_KEY: ${SUPABASE_KEY:-}
  # Kafka
  KAFKA_BROKERS: kafka:9092
  KAFKA_GROUP_ID: goat-prediction
  # API Gateway
  API_GATEWAY_URL: http://api-gateway:8000
  # Frontend
  FRONTEND_URL: ${FRONTEND_URL:-http://localhost:3000}
  # ML Service
  ML_SERVICE_URL: http://prediction-engine:8001

x-common-volumes: &common-volumes
  - shared-data:/shared/data
  - shared-logs:/shared/logs
  - shared-cache:/shared/cache
  - shared-models:/shared/models

x-common-networks: &common-networks
  networks:
    - goat-network
    - monitoring-network

x-common-deploy: &common-deploy
  deploy:
    resources:
      limits:
        cpus: '${DOCKER_CPU_LIMIT:-2}'
        memory: ${DOCKER_MEMORY_LIMIT:-4G}
      reservations:
        cpus: '${DOCKER_CPU_RESERVATION:-0.5}'
        memory: ${DOCKER_MEMORY_RESERVATION:-1G}
    restart_policy:
      condition: on-failure
      delay: 5s
      max_attempts: 3
      window: 120s
    update_config:
      parallelism: 1
      delay: 10s
      order: stop-first

services:
  # ============================================
  # DATABASE SERVICES
  # ============================================

  postgres:
    image: postgres:16-alpine
    container_name: goat-postgres
    restart: unless-stopped
    <<: *common-networks
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-goat_user}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-goat_password}
      - POSTGRES_DB=${POSTGRES_DB:-goat_prediction}
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
      - POSTGRES_HOST_AUTH_METHOD=scram-sha-256
      - POSTGRES_INITDB_WALDIR=/var/lib/postgresql/data/pg_wal
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./database/supabase/migrations:/docker-entrypoint-initdb.d
      - ./scripts/db/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-goat_user}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    <<: *common-deploy
    command: >
      postgres
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=4MB
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c max_connections=200
      -c shared_preload_libraries='pg_stat_statements'

  timescaledb:
    image: timescale/timescaledb:latest-pg16
    container_name: goat-timescaledb
    restart: unless-stopped
    <<: *common-networks
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-goat_user}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-goat_password}
      - POSTGRES_DB=${POSTGRES_DB:-goat_timeseries}
    volumes:
      - timescaledb-data:/var/lib/postgresql/data
      - ./database/timescaledb/hypertables:/docker-entrypoint-initdb.d
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-goat_user}"]
      interval: 10s
      timeout: 5s
      retries: 5
    <<: *common-deploy
    command: >
      postgres
      -c shared_buffers=512MB
      -c effective_cache_size=2GB
      -c maintenance_work_mem=128MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=500
      -c random_page_cost=1.1
      -c effective_io_concurrency=300
      -c work_mem=8MB
      -c min_wal_size=2GB
      -c max_wal_size=8GB
      -c max_connections=300

  redis:
    image: redis:7-alpine
    container_name: goat-redis
    restart: unless-stopped
    <<: *common-networks
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD:-goat_redis_pass}
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec
      --save 900 1
      --save 300 10
      --save 60 10000
      --loglevel warning
    volumes:
      - redis-data:/data
      - ./database/redis/redis.conf:/usr/local/etc/redis/redis.conf
      - ./database/redis/scripts:/scripts
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    <<: *common-deploy

  redis-cluster:
    image: redis:7-alpine
    container_name: goat-redis-cluster
    restart: unless-stopped
    <<: *common-networks
    command: >
      redis-server
      --cluster-enabled yes
      --cluster-config-file nodes.conf
      --cluster-node-timeout 5000
      --cluster-announce-ip redis-cluster
      --cluster-announce-port 6379
      --cluster-announce-bus-port 16379
      --appendonly yes
      --requirepass ${REDIS_CLUSTER_PASSWORD:-goat_cluster_pass}
    volumes:
      - redis-cluster-data:/data
    ports:
      - "6380:6379"
      - "16380:16379"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    <<: *common-deploy

  # ============================================
  # MESSAGE QUEUE & STREAMING
  # ============================================

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: goat-zookeeper
    restart: unless-stopped
    <<: *common-networks
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SYNC_LIMIT: 5
      ZOOKEEPER_INIT_LIMIT: 10
      ZOOKEEPER_MAX_CLIENT_CNXNS: 0
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD", "bash", "-c", "echo ruok | nc localhost 2181"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: goat-kafka
    restart: unless-stopped
    <<: *common-networks
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 10737418240
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_CLEANUP_POLICY: delete
      KAFKA_NUM_PARTITIONS: 10
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_COMPRESSION_TYPE: snappy
    volumes:
      - kafka-data:/var/lib/kafka/data
    ports:
      - "9092:9092"
    healthcheck:
      test: ["CMD", "kafka-topics", "--list", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: goat-kafka-ui
    restart: unless-stopped
    <<: *common-networks
    depends_on:
      - kafka
    environment:
      KAFKA_CLUSTERS_0_NAME: goat-prediction
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      DYNAMIC_CONFIG_ENABLED: "true"
    ports:
      - "8080:8080"
    <<: *common-deploy

  # ============================================
  # API GATEWAY & BACKEND SERVICES
  # ============================================

  api-gateway:
    build:
      context: ./backend/api-gateway
      dockerfile: Dockerfile
      args:
        NODE_ENV: ${NODE_ENV:-production}
        API_VERSION: ${API_VERSION:-1.0.0}
    container_name: goat-api-gateway
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy
    environment:
      - API_PORT=8000
      - API_HOST=0.0.0.0
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost:3000,http://frontend:3000}
      - RATE_LIMIT_WINDOW_MS=900000
      - RATE_LIMIT_MAX_REQUESTS=1000
      - JWT_SECRET=${JWT_SECRET:-goat_jwt_super_secret_key_2024}
      - JWT_EXPIRES_IN=7d
      - API_KEY_HEADER=X-API-Key
      - API_KEYS=${API_KEYS:-default_api_key_123}
    volumes:
      - ./backend/api-gateway:/app
      - ./backend/api-gateway/logs:/app/logs
      - shared-logs:/app/shared/logs
      - /var/run/docker.sock:/var/run/docker.sock:ro
    ports:
      - "8000:8000"
      - "8001:8001" # Debug port
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    <<: *common-deploy
    command: >
      sh -c "
        alembic upgrade head &&
        uvicorn src.main:app
        --host 0.0.0.0
        --port 8000
        --workers ${API_WORKERS:-4}
        --limit-concurrency ${API_CONCURRENCY:-1000}
        --limit-max-requests ${API_MAX_REQUESTS:-10000}
        --backlog ${API_BACKLOG:-2048}
        --timeout-keep-alive ${API_KEEP_ALIVE:-30}
        --log-level ${LOG_LEVEL:-info}
      "

  auth-service:
    build:
      context: ./backend/auth-service
      dockerfile: Dockerfile
    container_name: goat-auth-service
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      - postgres
      - redis
    environment:
      - SERVICE_PORT=8002
      - SERVICE_NAME=auth
      - AUTH_SECRET_KEY=${AUTH_SECRET_KEY:-auth_super_secret_2024}
      - OTP_EXPIRY_MINUTES=10
      - MAX_LOGIN_ATTEMPTS=5
      - LOCKOUT_DURATION_MINUTES=15
      - MFA_ENABLED=${MFA_ENABLED:-true}
    volumes:
      - ./backend/auth-service:/app
      - shared-logs:/app/logs
    ports:
      - "8002:8002"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  user-service:
    build:
      context: ./backend/user-service
      dockerfile: Dockerfile
    container_name: goat-user-service
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      - postgres
      - redis
    environment:
      - SERVICE_PORT=8003
      - SERVICE_NAME=user
      - USER_PROFILE_IMAGES_DIR=/app/uploads/profiles
      - MAX_UPLOAD_SIZE=5242880
      - ALLOWED_IMAGE_TYPES=image/jpeg,image/png,image/webp
    volumes:
      - ./backend/user-service:/app
      - user-uploads:/app/uploads
      - shared-logs:/app/logs
    ports:
      - "8003:8003"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  notification-service:
    build:
      context: ./backend/notification-service
      dockerfile: Dockerfile
    container_name: goat-notification-service
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      - redis
      - kafka
    environment:
      - SERVICE_PORT=8004
      - SERVICE_NAME=notification
      - NOTIFICATION_QUEUE=notifications
      - EMAIL_ENABLED=${EMAIL_ENABLED:-true}
      - SMS_ENABLED=${SMS_ENABLED:-false}
      - PUSH_ENABLED=${PUSH_ENABLED:-true}
      - WEBHOOK_ENABLED=${WEBHOOK_ENABLED:-true}
      - SMTP_HOST=${SMTP_HOST:-smtp.gmail.com}
      - SMTP_PORT=${SMTP_PORT:-587}
      - SMTP_USER=${SMTP_USER:-}
      - SMTP_PASS=${SMTP_PASS:-}
    volumes:
      - ./backend/notification-service:/app
      - shared-logs:/app/logs
      - notification-templates:/app/templates
    ports:
      - "8004:8004"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  subscription-service:
    build:
      context: ./backend/subscription-service
      dockerfile: Dockerfile
    container_name: goat-subscription-service
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      - postgres
      - redis
    environment:
      - SERVICE_PORT=8005
      - SERVICE_NAME=subscription
      - STRIPE_SECRET_KEY=${STRIPE_SECRET_KEY:-}
      - STRIPE_WEBHOOK_SECRET=${STRIPE_WEBHOOK_SECRET:-}
      - SUBSCRIPTION_PLANS=${SUBSCRIPTION_PLANS:-basic,premium,vip}
      - TRIAL_DAYS=7
      - GRACE_PERIOD_DAYS=3
    volumes:
      - ./backend/subscription-service:/app
      - shared-logs:/app/logs
    ports:
      - "8005:8005"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  # ============================================
  # ML & PREDICTION ENGINE
  # ============================================

  prediction-engine:
    build:
      context: ./backend/prediction-engine
      dockerfile: Dockerfile
      args:
        PYTHON_VERSION: 3.11
        TORCH_VERSION: 2.1.0
        TENSORFLOW_VERSION: 2.14.0
        CUDA_VERSION: 11.8
    container_name: goat-prediction-engine
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      - postgres
      - timescaledb
      - redis
      - redis-cluster
      - kafka
    environment:
      - SERVICE_PORT=8006
      - SERVICE_NAME=prediction-engine
      - ML_MODELS_DIR=/app/models
      - ML_DATA_DIR=/app/data
      - ML_CACHE_DIR=/app/cache
      - ML_LOG_DIR=/app/logs
      - ML_BATCH_SIZE=32
      - ML_MAX_WORKERS=4
      - ML_GPU_ENABLED=${ML_GPU_ENABLED:-false}
      - ML_GPU_DEVICE=${ML_GPU_DEVICE:-0}
      - ML_PREDICTION_TIMEOUT=30
      - ML_TRAINING_TIMEOUT=3600
      - ML_MODEL_UPDATE_INTERVAL=3600
      - ML_FEATURE_STORE_URL=http://feature-store:8080
    volumes:
      - ./backend/prediction-engine:/app
      - ml-models:/app/models
      - ml-data:/app/data
      - ml-cache:/app/cache
      - shared-logs:/app/logs
      - shared-models:/shared/models
      - /dev/nvidia0:/dev/nvidia0 # GPU passthrough
      - /dev/nvidiactl:/dev/nvidiactl
      - /dev/nvidia-uvm:/dev/nvidia-uvm
    ports:
      - "8006:8006"
      - "8007:8007" # MLflow tracking
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; socket.create_connection(('localhost', 8006), timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    <<: *common-deploy
    deploy:
      resources:
        limits:
          cpus: '${ML_CPU_LIMIT:-4}'
          memory: ${ML_MEMORY_LIMIT:-16G}
          devices:
            - driver: nvidia
              count: ${ML_GPU_COUNT:-1}
              capabilities: [gpu]
        reservations:
          cpus: '${ML_CPU_RESERVATION:-1}'
          memory: ${ML_MEMORY_RESERVATION:-4G}
    command: >
      sh -c "
        python src/main.py
        --port 8006
        --workers ${ML_WORKERS:-2}
        --log-level ${LOG_LEVEL:-info}
        --dev-mode ${DEV_MODE:-false}
      "

  feature-store:
    build:
      context: ./mlops/feature-store
      dockerfile: Dockerfile
    container_name: goat-feature-store
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      - postgres
      - redis
    environment:
      - SERVICE_PORT=8081
      - SERVICE_NAME=feature-store
      - FEAST_REGISTRY_PATH=/app/registry.db
      - FEAST_PROJECT=goat_prediction
      - FEAST_TELEMETRY=false
    volumes:
      - feature-store-data:/app/data
      - feature-store-registry:/app/registry
      - ./mlops/feature-store/feast:/app/feature_repo
    ports:
      - "8081:8081"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    container_name: goat-mlflow
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      - postgres
    environment:
      - MLFLOW_PORT=5000
      - MLFLOW_HOST=0.0.0.0
      - MLFLOW_BACKEND_STORE_URI=postgresql://${POSTGRES_USER:-goat_user}:${POSTGRES_PASSWORD:-goat_password}@postgres:5432/mlflow
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=/app/artifacts
      - MLFLOW_SERVE_ARTIFACTS=true
      - MLFLOW_ARTIFACTS_DESTINATION=/app/artifacts
    volumes:
      - mlflow-artifacts:/app/artifacts
      - ./mlops/model-registry/mlflow:/app/mlflow
    ports:
      - "5000:5000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri postgresql://${POSTGRES_USER:-goat_user}:${POSTGRES_PASSWORD:-goat_password}@postgres:5432/mlflow
      --default-artifact-root /app/artifacts
      --serve-artifacts
      --artifacts-destination /app/artifacts

  # ============================================
  # DATA PIPELINE & PROCESSING
  # ============================================

  airflow-webserver:
    image: apache/airflow:2.7.3
    container_name: goat-airflow-webserver
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      - postgres
      - redis
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-goat_user}:${POSTGRES_PASSWORD:-goat_password}@postgres:5432/airflow
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER:-goat_user}:${POSTGRES_PASSWORD:-goat_password}@postgres:5432/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://:${REDIS_PASSWORD:-goat_redis_pass}@redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY:-}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=10
      - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=30
      - _AIRFLOW_WWW_USER_USERNAME=${AIRFLOW_USERNAME:-admin}
      - _AIRFLOW_WWW_USER_PASSWORD=${AIRFLOW_PASSWORD:-admin}
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
    volumes:
      - airflow-dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - airflow-plugins:/opt/airflow/plugins
      - ./data-pipeline/batch/airflow/dags:/opt/airflow/dags
      - ./data-pipeline/batch/airflow/plugins:/opt/airflow/plugins
    ports:
      - "8082:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    <<: *common-deploy
    command: webserver

  airflow-scheduler:
    image: apache/airflow:2.7.3
    container_name: goat-airflow-scheduler
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      - postgres
      - redis
      - airflow-webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-goat_user}:${POSTGRES_PASSWORD:-goat_password}@postgres:5432/airflow
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER:-goat_user}:${POSTGRES_PASSWORD:-goat_password}@postgres:5432/airflow
      - AIRFLOW__CELERY__BROKER_URL=redis://:${REDIS_PASSWORD:-goat_redis_pass}@redis:6379/0
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY:-}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    volumes:
      - airflow-dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - airflow-plugins:/opt/airflow/plugins
      - ./data-pipeline/batch/airflow/dags:/opt/airflow/dags
      - ./data-pipeline/batch/airflow/plugins:/opt/airflow/plugins
    <<: *common-deploy
    command: scheduler

  flink-jobmanager:
    image: flink:1.18.0-scala_2.12
    container_name: goat-flink-jobmanager
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      - kafka
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - FLINK_PROPERTIES=jobmanager.rpc.address: flink-jobmanager
      - PARALLELISM_DEFAULT=4
      - TASK_MANAGER_NUMBER_OF_TASK_SLOTS=4
    ports:
      - "8083:8083"
    <<: *common-deploy
    command: jobmanager
    volumes:
      - flink-data:/opt/flink/data
      - ./data-pipeline/real-time/flink/jobs:/opt/flink/jobs

  flink-taskmanager:
    image: flink:1.18.0-scala_2.12
    container_name: goat-flink-taskmanager
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      - flink-jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - FLINK_PROPERTIES=jobmanager.rpc.address: flink-jobmanager;taskmanager.numberOfTaskSlots: 4
    <<: *common-deploy
    command: taskmanager
    volumes:
      - flink-data:/opt/flink/data
      - ./data-pipeline/real-time/flink/jobs:/opt/flink/jobs

  # ============================================
  # FRONTEND SERVICES
  # ============================================

  frontend:
    build:
      context: ./frontend/web-app
      dockerfile: Dockerfile
      args:
        NODE_ENV: ${NODE_ENV:-production}
        NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://localhost:8000}
        NEXT_PUBLIC_WS_URL: ${NEXT_PUBLIC_WS_URL:-ws://localhost:8000}
    container_name: goat-frontend
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      - api-gateway
      - prediction-engine
    environment:
      - NODE_ENV=${NODE_ENV:-production}
      - PORT=3000
      - HOSTNAME=0.0.0.0
      - NEXT_TELEMETRY_DISABLED=1
      - NEXT_PUBLIC_APP_VERSION=${APP_VERSION:-1.0.0}
      - NEXT_PUBLIC_SENTRY_DSN=${NEXT_PUBLIC_SENTRY_DSN:-}
      - NEXT_PUBLIC_GA_MEASUREMENT_ID=${NEXT_PUBLIC_GA_MEASUREMENT_ID:-}
    volumes:
      - ./frontend/web-app:/app
      - /app/node_modules
      - /app/.next
    ports:
      - "3000:3000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    <<: *common-deploy
    command: >
      sh -c "
        npm run build &&
        npm run start
      "

  admin-dashboard:
    build:
      context: ./frontend/admin-dashboard
      dockerfile: Dockerfile
    container_name: goat-admin-dashboard
    restart: unless-stopped
    <<: [*common-networks, *common-env]
    depends_on:
      - api-gateway
      - prediction-engine
    environment:
      - NODE_ENV=${NODE_ENV:-production}
      - PORT=3001
      - REACT_APP_API_URL=http://api-gateway:8000
      - REACT_APP_ADMIN_SECRET=${ADMIN_SECRET:-admin_super_secret_2024}
    volumes:
      - ./frontend/admin-dashboard:/app
      - /app/node_modules
      - /app/build
    ports:
      - "3001:3001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  # ============================================
  # MONITORING & OBSERVABILITY
  # ============================================

  prometheus:
    image: prom/prometheus:latest
    container_name: goat-prometheus
    restart: unless-stopped
    <<: *common-networks
    volumes:
      - prometheus-data:/prometheus
      - ./infrastructure/monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./infrastructure/monitoring/prometheus/rules:/etc/prometheus/rules
      - ./infrastructure/monitoring/prometheus/alerts:/etc/prometheus/alerts
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  grafana:
    image: grafana/grafana:latest
    container_name: goat-grafana
    restart: unless-stopped
    <<: *common-networks
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel,grafana-clock-panel
      - GF_FEATURE_TOGGLES_ENABLE=publicDashboards
    volumes:
      - grafana-data:/var/lib/grafana
      - ./infrastructure/monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./infrastructure/monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    ports:
      - "3002:3000"
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  loki:
    image: grafana/loki:latest
    container_name: goat-loki
    restart: unless-stopped
    <<: *common-networks
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - loki-data:/loki
      - ./infrastructure/monitoring/loki/loki-config.yaml:/etc/loki/local-config.yaml
    ports:
      - "3100:3100"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  tempo:
    image: grafana/tempo:latest
    container_name: goat-tempo
    restart: unless-stopped
    <<: *common-networks
    command: -config.file=/etc/tempo.yaml
    volumes:
      - tempo-data:/tmp/tempo
      - ./infrastructure/monitoring/tempo/tempo.yaml:/etc/tempo.yaml
    ports:
      - "3200:3200"
      - "4317:4317"
      - "4318:4318"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3200/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  alertmanager:
    image: prom/alertmanager:latest
    container_name: goat-alertmanager
    restart: unless-stopped
    <<: *common-networks
    volumes:
      - alertmanager-data:/alertmanager
      - ./infrastructure/monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    ports:
      - "9093:9093"
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: goat-jaeger
    restart: unless-stopped
    <<: *common-networks
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - LOG_LEVEL=debug
    ports:
      - "16686:16686"
      - "4317:4317"
      - "4318:4318"
      - "6831:6831/udp"
      - "6832:6832/udp"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:16686/"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  # ============================================
  # UTILITY SERVICES
  # ============================================

  nginx:
    image: nginx:alpine
    container_name: goat-nginx
    restart: unless-stopped
    <<: *common-networks
    depends_on:
      - frontend
      - api-gateway
      - admin-dashboard
      - grafana
    volumes:
      - ./infrastructure/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./infrastructure/nginx/sites-available:/etc/nginx/sites-available
      - ./infrastructure/nginx/ssl:/etc/nginx/ssl
      - nginx-logs:/var/log/nginx
      - static-files:/var/www/static
    ports:
      - "80:80"
      - "443:443"
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  portainer:
    image: portainer/portainer-ce:latest
    container_name: goat-portainer
    restart: unless-stopped
    <<: *common-networks
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - portainer-data:/data
    ports:
      - "9000:9000"
    command: -H unix:///var/run/docker.sock
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/api/status"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  mailhog:
    image: mailhog/mailhog:latest
    container_name: goat-mailhog
    restart: unless-stopped
    <<: *common-networks
    ports:
      - "1025:1025"
      - "8025:8025"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8025/"]
      interval: 30s
      timeout: 10s
      retries: 3
    <<: *common-deploy

  # ============================================
  # NETWORKS & VOLUMES
  # ============================================

networks:
  goat-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
    name: goat-network
  monitoring-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/16
          gateway: 172.21.0.1
    name: monitoring-network

volumes:
  # Database volumes
  postgres-data:
    driver: local
    name: goat-postgres-data
  timescaledb-data:
    driver: local
    name: goat-timescaledb-data
  redis-data:
    driver: local
    name: goat-redis-data
  redis-cluster-data:
    driver: local
    name: goat-redis-cluster-data
  
  # Message queue volumes
  zookeeper-data:
    driver: local
    name: goat-zookeeper-data
  zookeeper-logs:
    driver: local
    name: goat-zookeeper-logs
  kafka-data:
    driver: local
    name: goat-kafka-data
  
  # ML volumes
  ml-models:
    driver: local
    name: goat-ml-models
  ml-data:
    driver: local
    name: goat-ml-data
  ml-cache:
    driver: local
    name: goat-ml-cache
  feature-store-data:
    driver: local
    name: goat-feature-store-data
  feature-store-registry:
    driver: local
    name: goat-feature-store-registry
  mlflow-artifacts:
    driver: local
    name: goat-mlflow-artifacts
  
  # Data pipeline volumes
  airflow-dags:
    driver: local
    name: goat-airflow-dags
  airflow-logs:
    driver: local
    name: goat-airflow-logs
  airflow-plugins:
    driver: local
    name: goat-airflow-plugins
  flink-data:
    driver: local
    name: goat-flink-data
  
  # Frontend volumes
  user-uploads:
    driver: local
    name: goat-user-uploads
  notification-templates:
    driver: local
    name: goat-notification-templates
  static-files:
    driver: local
    name: goat-static-files
  
  # Monitoring volumes
  prometheus-data:
    driver: local
    name: goat-prometheus-data
  grafana-data:
    driver: local
    name: goat-grafana-data
  loki-data:
    driver: local
    name: goat-loki-data
  tempo-data:
    driver: local
    name: goat-tempo-data
  alertmanager-data:
    driver: local
    name: goat-alertmanager-data
  
  # Utility volumes
  nginx-logs:
    driver: local
    name: goat-nginx-logs
  portainer-data:
    driver: local
    name: goat-portainer-data
  
  # Shared volumes
  shared-data:
    driver: local
    name: goat-shared-data
  shared-logs:
    driver: local
    name: goat-shared-logs
  shared-cache:
    driver: local
    name: goat-shared-cache
  shared-models:
    driver: local
    name: goat-shared-models

# ============================================
# ENVIRONMENT FILE
# ============================================

# Usage:
# docker-compose --env-file .env.production up -d
# docker-compose --env-file .env.development up -d

# Environment variables should be defined in:
# - .env.production (for production)
# - .env.development (for development)
# - .env.staging (for staging)
